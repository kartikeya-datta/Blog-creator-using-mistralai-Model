{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral-Powered Blog Generator: Code Explanation\n",
    "\n",
    "This notebook explains the implementation of a Streamlit web application that uses the Mistral-7B language model to generate blog posts. The application allows users to enter a topic, and the model generates a blog post about that topic. Users can also adjust generation parameters and download the generated blog as a Word document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Let's break down the libraries being imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Web application framework\n",
    "import streamlit as st\n",
    "\n",
    "# Hugging Face Transformers libraries for the language model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# PyTorch for tensor operations\n",
    "import torch\n",
    "\n",
    "# For suppressing warnings\n",
    "import warnings\n",
    "\n",
    "# For creating in-memory files\n",
    "from io import BytesIO\n",
    "\n",
    "# For creating Word documents\n",
    "from docx import Document\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streamlit Page Configuration\n",
    "\n",
    "Here we configure the Streamlit page appearance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Configuring the page title, icon, layout, and sidebar state\n",
    "st.set_page_config(\n",
    "    page_title=\"Mistral Blog Generator\",\n",
    "    page_icon=\"üìù\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for styling the app\n",
    "st.markdown(\"\"\"\n",
    "    <style>\n",
    "    .main .block-container {\n",
    "        padding-top: 2rem;\n",
    "    }\n",
    "    .stButton>button {\n",
    "        width: 100%;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Set the app title\n",
    "st.title(\"üìù Mistral-Powered Blog Generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the Mistral Model\n",
    "\n",
    "This function loads the Mistral-7B language model. The `@st.cache_resource` decorator ensures the model is only loaded once and then cached, saving time and resources on subsequent runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "@st.cache_resource(show_spinner=True)\n",
    "def load_model():\n",
    "    with st.spinner(\"Loading Mistral-7B-Instruct model. This may take a few minutes...\"):\n",
    "        # Load the tokenizer for Mistral-7B-Instruct\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "        \n",
    "        # Load the model - using float16 precision and automatic device mapping for efficiency\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
    "            torch_dtype=torch.float16,  # Use half-precision to reduce memory usage\n",
    "            device_map=\"auto\"          # Let the library decide how to map model to available hardware\n",
    "        )\n",
    "        \n",
    "        # Return a text generation pipeline that combines the model and tokenizer\n",
    "        return pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Blog Generation Function\n",
    "\n",
    "This function takes a topic and generation parameters, then creates a prompt and generates blog content using the Mistral model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def generate_blog(topic, max_length=512, temperature=0.7, top_p=0.9):\n",
    "    # Create a prompt with the Mistral instruction format [INST] ... [/INST]\n",
    "    prompt = f\"[INST] Write a detailed blog post about {topic}. [/INST]\"\n",
    "    \n",
    "    # Generate the blog post\n",
    "    result = generator(\n",
    "        prompt, \n",
    "        max_length=max_length,     # Maximum length of the generated text\n",
    "        do_sample=True,            # Use sampling instead of greedy decoding\n",
    "        temperature=temperature,   # Controls randomness (higher = more random)\n",
    "        top_p=top_p                # Nucleus sampling parameter (filters token probabilities)\n",
    "    )[0][\"generated_text\"]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Document Creation Function\n",
    "\n",
    "This function creates a Word document from the generated blog content, allowing users to download their blog post:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_word_doc(content):\n",
    "    \"\"\"\n",
    "    Creates a Word document from the generated blog content.\n",
    "    \"\"\"\n",
    "    # Create a new Word document\n",
    "    doc = Document()\n",
    "    \n",
    "    # Add a title heading\n",
    "    doc.add_heading('Generated Blog Post', 0)\n",
    "    \n",
    "    # Add the blog content as a paragraph\n",
    "    doc.add_paragraph(content)\n",
    "    \n",
    "    # Save the document to a BytesIO object (in-memory file)\n",
    "    bio = BytesIO()\n",
    "    doc.save(bio)\n",
    "    bio.seek(0)  # Reset the file pointer to the beginning\n",
    "    \n",
    "    return bio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main UI Function\n",
    "\n",
    "The main function creates the Streamlit user interface with input controls and handles the blog generation workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def main():\n",
    "    # Create the sidebar with generation settings\n",
    "    st.sidebar.title(\"‚öôÔ∏è Generation Settings\")\n",
    "    mode = st.sidebar.radio(\"Mode\", [\"Simple\", \"Advanced\"])\n",
    "    \n",
    "    # Default parameter values\n",
    "    max_length = 512\n",
    "    temperature = 0.7\n",
    "    top_p = 0.9\n",
    "    \n",
    "    # Show advanced settings if selected\n",
    "    if mode == \"Advanced\":\n",
    "        max_length = st.sidebar.slider(\"Maximum Length\", min_value=256, max_value=1024, value=512, step=128)\n",
    "        temperature = st.sidebar.slider(\"Temperature\", min_value=0.1, max_value=1.5, value=0.7, step=0.1)\n",
    "        top_p = st.sidebar.slider(\"Top-p\", min_value=0.1, max_value=1.0, value=0.9, step=0.1)\n",
    "    \n",
    "    # Information about Mistral-7B in the sidebar\n",
    "    with st.sidebar.expander(\"‚ÑπÔ∏è About Mistral-7B\"):\n",
    "        st.markdown(\"**Mistral-7B-Instruct** is a 7 billion parameter language model fine-tuned for following instructions.\")\n",
    "    \n",
    "    # Main input for blog topic\n",
    "    topic = st.text_input(\"Enter a blog topic:\", \"How to Learn Machine Learning\")\n",
    "    \n",
    "    # Generate button and handling\n",
    "    if st.button(\"Generate Blog\", type=\"primary\"):\n",
    "        if topic.strip():\n",
    "            with st.spinner(\"Generating blog... please wait ‚è≥\"):\n",
    "                # Load model if not already loaded\n",
    "                if 'generator' not in st.session_state:\n",
    "                    st.session_state.generator = load_model()\n",
    "                    \n",
    "                generator = st.session_state.generator\n",
    "                # Generate the blog\n",
    "                blog_content = generate_blog(topic, max_length, temperature, top_p)\n",
    "                \n",
    "                # Display the generated blog\n",
    "                st.subheader(f\"üß† Generated Blog: {topic}\")\n",
    "                st.markdown(blog_content)\n",
    "                \n",
    "                # Create a download button for the Word document\n",
    "                word_doc = create_word_doc(blog_content)\n",
    "                st.download_button(\n",
    "                    label=\"Download Blog Post (Word Format)\",\n",
    "                    data=word_doc,\n",
    "                    file_name=f\"{topic.replace(' ', '_').lower()}_blog.docx\",\n",
    "                    mime=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "                )\n",
    "        else:\n",
    "            st.warning(\"Please enter a valid topic!\")\n",
    "    \n",
    "    st.caption(\"Note: Blog generation may take 10-30 seconds depending on length and hardware.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Application Entry Point\n",
    "\n",
    "Finally, we need to call the main function when the script is run directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Components and Concepts\n",
    "\n",
    "### 1. Streamlit Framework\n",
    "- **Streamlit** is used to create the web interface with minimal code\n",
    "- Components like text inputs, sliders, buttons, and spinners create an interactive UI\n",
    "- `st.session_state` maintains the loaded model between reruns\n",
    "\n",
    "### 2. Hugging Face Transformers\n",
    "- **AutoTokenizer** converts text to tokens that the model can understand\n",
    "- **AutoModelForCausalLM** loads the Mistral-7B language model\n",
    "- **pipeline** simplifies working with the model through a unified interface\n",
    "\n",
    "### 3. Generation Parameters\n",
    "- **max_length**: Controls how long the generated blog can be\n",
    "- **temperature**: Controls randomness/creativity (higher = more random)\n",
    "- **top_p**: Controls diversity via nucleus sampling\n",
    "\n",
    "### 4. Performance Optimizations\n",
    "- Using `torch.float16` reduces memory requirements by using half-precision\n",
    "- `device_map=\"auto\"` optimizes model loading across available GPUs/CPU\n",
    "- `@st.cache_resource` prevents reloading the model on each interaction\n",
    "\n",
    "### 5. Document Generation\n",
    "- The `python-docx` library creates Word documents\n",
    "- `BytesIO` creates in-memory files for download without disk storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Application\n",
    "\n",
    "To run this Streamlit app:\n",
    "\n",
    "1. Install the required packages:\n",
    "```bash\n",
    "pip install streamlit transformers torch python-docx\n",
    "```\n",
    "\n",
    "2. Save the code to a file (e.g., `blog_generator.py`)\n",
    "\n",
    "3. Run the Streamlit app:\n",
    "```bash\n",
    "streamlit run blog_generator.py\n",
    "```\n",
    "\n",
    "4. A browser window will open with the application\n",
    "\n",
    "**Note**: The Mistral-7B model requires significant resources. You'll need a computer with at least 16GB RAM, and ideally a GPU with 8+ GB VRAM for reasonable performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
